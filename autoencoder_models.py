import torch
import torch.nn as nn
import bpdb

from generic_nn_modules import Flatten, GenericUnflatten


class VanillaAutoenc(nn.Module):
    """For 64x64 images"""
    def __init__(self, n_channels=3, z_dim=512, categorical=True):
        super(VanillaAutoenc, self).__init__()
        self.encoder = nn.Sequential(
            # Input batchsize x n_channels x 64 x 64
            nn.Conv2d(n_channels, n_channels, kernel_size=4, stride=1, padding='same'),
            nn.LeakyReLU(),
            nn.BatchNorm2d(n_channels),
            nn.Conv2d(n_channels, n_channels, kernel_size=4, stride=1, padding='same'),
            nn.LeakyReLU(),
            nn.BatchNorm2d(n_channels),
            nn.Conv2d(n_channels, n_channels * 2, kernel_size=2, stride=2, padding=0),
            nn.LeakyReLU(),
            nn.BatchNorm2d(n_channels * 2),
            nn.Conv2d(n_channels * 2, n_channels * 4, kernel_size=2, stride=2, padding=0),
            nn.LeakyReLU(),
            nn.BatchNorm2d(n_channels * 4),
            nn.Conv2d(n_channels * 4, n_channels * 8, kernel_size=2, stride=2, padding=0),
            nn.LeakyReLU(),
            nn.BatchNorm2d(n_channels * 8),
            nn.Conv2d(n_channels * 8, n_channels * 8, kernel_size=2, stride=2, padding=0),
            nn.LeakyReLU(),
            nn.BatchNorm2d(n_channels * 8),
            nn.Conv2d(n_channels * 8, z_dim, kernel_size=2, stride=2, padding=0),
            nn.LeakyReLU(),
            nn.BatchNorm2d(z_dim),
            Flatten(),
            nn.Linear(z_dim*4, z_dim),
            nn.LeakyReLU(),
            nn.BatchNorm1d(z_dim),
            nn.Linear(z_dim, z_dim),
            nn.LeakyReLU(),
            nn.BatchNorm1d(z_dim),
        )

        decoder_layers = [
            nn.Linear(z_dim, z_dim),
            nn.LeakyReLU(),
            nn.BatchNorm1d(z_dim),
            nn.Linear(z_dim, z_dim * 4),
            nn.LeakyReLU(),
            nn.BatchNorm1d(z_dim * 4),
            GenericUnflatten((z_dim, 2, 2)),
            nn.ConvTranspose2d(z_dim, n_channels * 8, kernel_size=2, stride=2, padding=0),
            nn.LeakyReLU(),
            nn.BatchNorm2d(n_channels * 8),
            nn.ConvTranspose2d(n_channels * 8, n_channels * 8, kernel_size=2, stride=2, padding=0),
            nn.LeakyReLU(),
            nn.BatchNorm2d(n_channels * 8),
            nn.ConvTranspose2d(n_channels * 8, n_channels * 4, kernel_size=2, stride=2, padding=0),
            nn.LeakyReLU(),
            nn.BatchNorm2d(n_channels * 4),
            nn.ConvTranspose2d(n_channels * 4, n_channels * 2, kernel_size=2, stride=2, padding=0),
            nn.LeakyReLU(),
            nn.BatchNorm2d(n_channels * 2),
            nn.ConvTranspose2d(n_channels * 2, n_channels, kernel_size=2, stride=2, padding=0),
            nn.LeakyReLU(),
            nn.BatchNorm2d(n_channels),
            nn.Conv2d(n_channels, n_channels, kernel_size=4, stride=1, padding='same'),
            nn.LeakyReLU(),
            nn.BatchNorm2d(n_channels),
            nn.Conv2d(n_channels, n_channels, kernel_size=4, stride=1, padding='same'),
            nn.LeakyReLU(),
            nn.BatchNorm2d(n_channels),
        ]

        if categorical:
            # Applies softmax to every channel
            # This makes sense if using one hot encoding
            decoder_layers.append(nn.Softmax(dim=1))
        else:
            decoder_layers.append(nn.Sigmoid)

        self.decoder = nn.Sequential(
            *decoder_layers
        )

    def forward(self, x):
        h = self.encoder(x)
        return self.decoder(h)


class VanillaDisc(nn.Module):
    """Attempts to determine if a sample from the latent dim is random
    gaussian, or generated by the generator"""

    def __init__(self, nz, extra_layers=2) -> None:
        super().__init__()
        layers = []
        for _ in range(extra_layers):
            layers.append(
            nn.Linear(nz, nz))
            layers.append(nn.LeakyReLU())
            layers.append(nn.BatchNorm1d(nz))

        self.main = nn.Sequential(*layers, 
                nn.Linear(nz, nz // 2),
                nn.LeakyReLU(),
                nn.BatchNorm1d(nz//2),
                nn.Linear(nz//2, nz//4),
                nn.LeakyReLU(),
                nn.BatchNorm1d(nz//4),
                nn.Linear(nz//4, 1),
                nn.Sigmoid()
                )


    def forward(self, input):
        return self.main(input)



class Decoder(nn.Module):
    """Generic decoder, with a single linear input layer, multiple
    ConvTranspose2d upscaling layers, and batch normalization. Works on a 64x64
    image output size,
    Outputs are scaled by the sigmoid function to be between zero and one.
    """
    def __init__(self, n_channels, z_dim):
        super().__init__()
        self.decoder = nn.Sequential(
            nn.Linear(z_dim, z_dim * 4),
            nn.LeakyReLU(),
            nn.BatchNorm1d(z_dim * 4),
            GenericUnflatten((z_dim, 2, 2)),
            nn.ConvTranspose2d(z_dim, n_channels * 8, kernel_size=2, stride=2, padding=0),
            nn.LeakyReLU(),
            nn.BatchNorm2d(n_channels * 8),
            nn.ConvTranspose2d(n_channels * 8, n_channels * 8, kernel_size=2, stride=2, padding=0),
            nn.LeakyReLU(),
            nn.BatchNorm2d(n_channels * 8),
            nn.ConvTranspose2d(n_channels * 8, n_channels * 4, kernel_size=2, stride=2, padding=0),
            nn.LeakyReLU(),
            nn.BatchNorm2d(n_channels * 4),
            nn.ConvTranspose2d(n_channels * 4, n_channels * 2, kernel_size=2, stride=2, padding=0),
            nn.LeakyReLU(),
            nn.BatchNorm2d(n_channels * 2),
            nn.ConvTranspose2d(n_channels * 2, n_channels, kernel_size=2, stride=2, padding=0),
            nn.LeakyReLU(),
            nn.BatchNorm2d(n_channels),
            nn.Conv2d(n_channels, n_channels, kernel_size=4, stride=1, padding=2),
            nn.LeakyReLU(),
            nn.BatchNorm2d(n_channels),
            nn.Conv2d(n_channels, n_channels, kernel_size=4, stride=1, padding=1),
            nn.Sigmoid(),
        )

    def forward(self, z):
        return self.decoder(z)


class VariationalEncoder(nn.Module):
    """
    Variational encoder, with a single output linear layer, and then another
    single linear layer producing either mu or sigma^2. This encoder works only
    with 64x64 input image resolution and a 128 latent dimension.
    """
    def __init__(self, n_channels, z_dim):
        super().__init__()
        self.encoder = nn.Sequential(
        # Input batchsize x n_channels x 64 x 64
        nn.Conv2d(n_channels, n_channels, kernel_size=4, stride=1, padding=1),
        nn.LeakyReLU(),
        nn.BatchNorm2d(n_channels),
        nn.Conv2d(n_channels, n_channels, kernel_size=4, stride=1, padding=2),
        nn.LeakyReLU(),
        nn.BatchNorm2d(n_channels),
        nn.Conv2d(n_channels, n_channels * 2, kernel_size=2, stride=2, padding=0),
        nn.LeakyReLU(),
        nn.BatchNorm2d(n_channels * 2),
        nn.Conv2d(n_channels * 2, n_channels * 4, kernel_size=2, stride=2, padding=0),
        nn.LeakyReLU(),
        nn.BatchNorm2d(n_channels * 4),
        nn.Conv2d(n_channels * 4, n_channels * 8, kernel_size=2, stride=2, padding=0),
        nn.LeakyReLU(),
        nn.BatchNorm2d(n_channels * 8),
        nn.Conv2d(n_channels * 8, n_channels * 8, kernel_size=2, stride=2, padding=0),
        nn.LeakyReLU(),
        nn.BatchNorm2d(n_channels * 8),
        nn.Conv2d(n_channels * 8, z_dim, kernel_size=2, stride=2, padding=0),
        nn.LeakyReLU(),
        nn.BatchNorm2d(z_dim),
        Flatten(),
        nn.Linear(z_dim*4, z_dim),
        nn.LeakyReLU(),
        nn.BatchNorm1d(z_dim),
        )

        self.mu_layer = nn.Linear(z_dim, z_dim)
        self.var_layer = nn.Linear(z_dim, z_dim)

    def forward(self, x):
        """Returns mu, var"""

        p_x = self.encoder(x)
        mu = self.mu_layer(p_x)
        var = self.var_layer(p_x)

        return mu, var
